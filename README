This repository represents the first attempt to develop sampling with PAPI.

Usage:
Calling the script "test_script" like so:
	./test_script ./src/event_lists/LISTOFEVENTSFORYOURARCHITECTURE
will try to sample every event available for the architecture at a frequency
specified in the file src/test_events.c

History:

6/22
about half of the events for the Skylake processor series are working with an i7-6820HQ on a lenovo
t460p with ubuntu 16.04 LTS

6/25
about to completely reorganize the directory and add a decent makefile system (maybe....)
here goes nothing...

6/27
As of right now it appears sampling only works for 19 of the specified 50 events
that PEBS is supposed to be supported for. It's unclear whether I'm using the
erf_event_open interface wrong or if these events just dont work.

As the code stands right now, it takes the INTEL name for sampling events and
feeds them into libpfm in order to generate perf_event_attr structures, which
are modified slightly before use. As it progresses, the code should take generic
PAPI sampling names; these can then be used to translate to the INTEL name,
which can be fed to the call to pfm_get_os_event_encoding.

7/3
Verified that about 15 more events are *working* at least for SKYLAKE. The
sampling frequency had previously been set to a value that was too large to
catch sampling results for these events because the events do not trigger as
frequently as common ones.

Currently working to develop a test program for the HLE and RTM metrics. These
features correspond to specific assembly instructions, and therefore need a
separate test.

7/9
Current sampling interface is safe only for serial programs (ie not multithreaded)
After trying to sample some results on linpack, the code became trapped in an
infinite loop. The PAPI_sampled events need to interface with the ESI system
for eventsets in order to make them thread safe.

7/10
After closely examining "A Prototype Sampling Interface for PAPI", I found the
solution to my multicore problem. About to switch the interface over to use
multiple perf_event_open for each core so all events are monitored.

7/10 (cont.)
Started the transition to multicore. Encountering some struggles:
	Can each thread use the same event handler?
		If so, how do I make sure that parse_record gets the MMAP associated
		with the correct file descriptor/interrupt.
			One solution is having a handler for every thread but this seems
			very clunky
			How to map file descriptor to MMAP

	Should the MMAPs be managed outside or inside the sample_init?
		I would think local modification makes sense to abstract complication
		of mmap from user.
		Back to issue number 1....

7/11
Devised an awful solution for MMAPing that is working on my computer for one
test program. It's all hard coded and really isnt a solution at all, I just
wanted to see it work and get me going in the right direction...

The current implementation has the user managing the mmaps with a lot of
convoluted PAPI side code just to make the MMAPs work for my machine.
	Tomorrow:
		Try moving the MMAPs back to the PAPI side as it is much easier than
		trying to track down memory addresses and page boundaries. PAPI_sample
		should always be able to find the address of the mmap and shouldn't have
		to do the weird conversion using the negative indexed array
		(stack grows down). Change the strcture mmap_info to have the fd as
		well. Check each fd against the fd in the array of structures. Be
		sure to keep a count as to how long the structure is (still not
		convinced this implementation is going to work).


		The other thing you could try is making a hash of fd to location of the
		mmap on the user side and pass that to PAPI_sample. It would solve the
		hard-coded offsets that I'm currently using for my machine. Still think
		option number 1 is probably better as I spent the entire day chasing
		around addresses for mmaps.

7/12
UPDATE!!!! I got multicore working pretty fast this morning. :) still need to
check results for "PESKY" events, but I think everything should be as before.

Seems as if all the normal events were working for the naive test. About to
push the code as it currently is. Currently working to see if the code is
working in linpack. This should test the multicore capabilities.

7/13
Testing with the base HPL.dat that linpack produces has revealed there is an
issue with the multithreaded sampling. linpack will occasionally just hang,
holding all of the threads its using at 100% CPU. The sampling interface
stops recording results and the code will hang forever.

I tracked down the error in the code that was causing linpack to hang forever.
The data array that was previously holding the values copied from the mmap
was being malloced inside of parse_record. This became an issue when the
signal handler was called when linpack was calling malloc. Linpack has the
memory lock but the signal handler also tries to take it via parse_record and
becomes stuck in deadlock. The solution is to move the malloc for that data
array into the initialization for PAPI_sample and then just pass the location
of the memory to the handler. In addition, the data buffer is memset to 0
at the end of every parse_record.

7/17
Ankle set back made me miss Monday tracking down healthcare. They always work
so quickly.... HA! Anyways, over the weekend I read that only certain actions
can be performed in a signal handler. The handler currently in effect is
technically undefined behavior but seems to be performing fine. Ask Vince about
this. (it was his code that originally had the behavior)

7/23
Much of my efforts last week were poured into the paper for the ESPT workshop.
I did figure out how to do latency events last week, which I have now worked
into the code.

7/24
Got linpack going on the lab SKYLAKE machine. It was a little tricky to specify
the statically linked libraries in the exact format mpicc wants them. Tomorrow,
test all events on SKYLAKE to affirm they work in different machines than just
mine. In addition, now that we have linpack for SKYLAKE statically compiled,
porting it to the Broadwell and Haswell machines should not be too challenging.
I can then beginning testing for those sets of events.
